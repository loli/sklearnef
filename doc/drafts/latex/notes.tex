\documentclass[10pt,a4paper]{article}
\usepackage[utf8]{inputenc}
\usepackage{amsmath}
\usepackage{amsfonts}
\usepackage{amssymb}
\usepackage{graphicx}
\author{Oskar Maier}
\title{Notes on \textit{sklearnef}}

\begin{document}

\maketitle

\section{Entropy (information theory)}
The entropy describes the average information content or information density of a message. A high entropy is often associated with maximum chaos / minimum order / pure chance / insecurity.
Entropy is defined as $H = -\sum_{c\in C}p_c \ln p_c$. Note that $0 \ln 0 = 0$, as $\lim_{x\to 0}x \ln x = 0$.

\subsection{Maximum entropy}
The maximum value the entropy can take is under a discrete normal distributions and amounts to $H_{max} = \ln N$.

\subsection{Entropy range}
$H \in [0, H_{max}]$, where a value of $0$ means perfectly ordered (one class with 100\% probability, all other without any) and $H_{max}$ signifies a discrete normal distribution.

\subsection{Normalizing the entropy}
$H_{norm} = \frac{H}{H_{max}} \leq 1$

\subsection{Entropy of a subset}
If $S_b \subset S_a$, then $H(S_b)$ can be $>$, $=$ or $<$ than $H(S_a)$.

\subsection{Information gain range based on entropy (information theory)}
Defined as $I = H(S) - \sum_{i\in{L,R}}\frac{|S_i|}{|S|}H(S_i)$, the information gain has a support of $I \in [0, \ln 2]$.


\section{Entropy (differential)}
Where the entropy (information theory describes discrete, the entropy (differential) describes continuous random variables. It can be defined as a measure of average surprisal of a random variable from a continuous probability distributions. Unlike the Shannon entropy, the differential entropy is not in general a good measure of uncertainty or information. For example, the differential entropy can be negative; also it is not invariant under continuous co-ordinate transformations.

Considering a random variable $X$ with a probability density function $f$ with support (definition range) of $\mathbb{X}$, then the entropy is defined as $h(X) = \int_{\mathbb{X}}f(x)\ln f(x) dx$.

\subsection{Notes on variable range}
The entropy is not scale invariant, hence the random variables should be normalized to the same range.
\textbf{Is this assured in the case of the Gaussian based entropy?!?!?}
The magnitude of the determinant value depends on the dimensionality of the multivariate Gaussian.

\subsection{Entropy for multivariate Gaussian}
If the random variable $X$ comes from a multivariate Gaussian with dimensionality $n$ mean $\mu$ and co-variance matrix $\Sigma$, then $h(X) = \frac{1}{2} \ln\left((2\pi e))^n \det{\Sigma}\right)$.

\subsection{Range of entropy for multivariate Gaussian}
The support for the entropy is $(-\inf, \inf)$.

\subsection{Maximization in the normal distribution}
A Gaussian variable has the largest entropy amongst all random variables of equal variance, or, alternatively, that the maximum entropy distribution under constraints of mean and variance is the Gaussian.
That means if $g(x)$ is a Gaussian PDF with mean $\mu$ and variance $\sigma^2$ and $f(x)$ an arbitrary PDF with the same variance, it holds that $h(g(x)) \geq h(f(X))$.

\subsection{Information gain range based on entropy (differential)}
Defined as $I = H(S) - \sum_{i\in{L,R}}\frac{|S_i|}{|S|}H(S_i)$, the information gain has a support of $I \in [-\inf, \inf]$.



\section{Positive entropy hack}
As the implemented classification tree variants in \textit{sklearn} are all based on the non-negative entropy (information gain), the \texttt{TreeBuilders} checks the entropy (here: \textit{impurity}) of each node against an upper threshold \texttt{MIN\_IMPURITY\_SPLIT} to determine whether to declare the node a leaf or to attempt another split.
Since the entropy (differential) covers the whole range of the real numbers, includings the negative ones, a hack has to be employed to avoid a premature leaf declaration. In skleanef, this is implemented by adding a constant \texttt{ENTROPY\_SHIFT} on the computed impurities in
\texttt{children\_impurity()} as well as \texttt{node\_impurity()}. Thus, the value of the entropy (differential) is shifted partly into the positive, resulting in \texttt{ENTROPY\_SHIFT -  MIN\_IMPURITY\_SPLIT} to become the de facto threshold point to decide between leaf and a further split.

\section{Minimal information gain hack}
\textit{Sklearn} offers with \texttt{max\_depth}, \texttt{min\_samples\_split}, \texttt{min\_samples\_leaf}, \texttt{min\_weight\_fraction\_leaf} and \texttt{max\_leaf\_nodes} a number of control parameters to restrict the tree growth. What it does not offer is a \texttt{min\_improvement} variable, albeit the information gain (information theory) has a fixed range of $I \in [0, \ln 2]$.

While this does not hold true for the information gain (differential), such a control parameter is more apt to control the tree growth in the case of fitting Gaussians to the data. Therefore, we introduce the \texttt{min\_improvement} parameter, which is passes to the \texttt{UnSupervisedClassificationCriterion}. To achieve the desired effect, the \texttt{impurity\_improvement} method is overwritten and in the case of a \texttt{improvement} $<$ \texttt{min\_split\_gain} the \texttt{-INFINITY} is returned in place of the impurity improvement, which de facto invalidates the current split.

\section{Achieving a integral value / CDF of 1 for the learned density function}

\section{Coping with singular co-variance matrices during training}

\section{Coping with singular co-variance matrices during application}

\section{Coping with numerical instability during logdet computation}


\end{document}